{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10adfbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def process_questions_config(filepath='data/questions.json'):\n",
    "    with open(filepath, 'r') as f:\n",
    "        questions_data = json.load(f)\n",
    "    answer_mappings = {}\n",
    "    num_options_map = {}\n",
    "    for qid, details in questions_data.items():\n",
    "        if details.get(\"scale\", False):\n",
    "            num_options_map[qid] = 10\n",
    "            answer_mappings[qid] = {details[\"options\"][0]: 1, details[\"options\"][-1]: 10}\n",
    "        else:\n",
    "            valid_options = [opt for opt in details[\"options\"] if opt.lower() != \"don't know\"]\n",
    "            num_options_map[qid] = len(valid_options)\n",
    "            answer_mappings[qid] = {option: i + 1 for i, option in enumerate(valid_options)}\n",
    "    return answer_mappings, num_options_map\n",
    "\n",
    "def get_demographic_mapping(year='2022'):\n",
    "    with open(\"data/chosen_cols_gemma_updated.json\", \"r\") as f:\n",
    "        persona_cols_json = json.load(f)\n",
    "\n",
    "    if year not in persona_cols_json['persona_cols']:\n",
    "        raise ValueError(f\"No demographic mapping found for year {year}\")\n",
    "    \n",
    "    year_mapping = persona_cols_json['persona_cols'][year]\n",
    "    \n",
    "    demographic_mapping = {}\n",
    "    for key, val in year_mapping.items():\n",
    "        col_name = val.split(\":\")[0].strip()\n",
    "        if key == 'sex':\n",
    "            final_key = 'gender'\n",
    "        elif key == 'education':\n",
    "            final_key = 'education_level'\n",
    "        else:\n",
    "            final_key = key\n",
    "        demographic_mapping[col_name] = final_key\n",
    "    return demographic_mapping\n",
    "\n",
    "def analyze_survey_alignment(\n",
    "    year='2022',\n",
    "    state='bengal',\n",
    "    metric_type='soft',\n",
    "    region_wise=False \n",
    "):\n",
    "    wvs_filepath=f'data/india/{year}/{year}_india_majority_answers_by_persona.csv'\n",
    "    filepath=f'llama_responses/most_frequent_answers_{state}.csv'\n",
    "    questions_filepath='data/questions.json'\n",
    "    \n",
    "    if metric_type not in ['hard', 'soft']:\n",
    "        raise ValueError(\"Metric type must be either 'hard' or 'soft'\")\n",
    "\n",
    "    answer_mappings_by_q, num_options_map = process_questions_config(questions_filepath)\n",
    "    flat_answer_mapping = {}\n",
    "    for q_map in answer_mappings_by_q.values():\n",
    "        flat_answer_mapping.update(q_map)\n",
    "\n",
    "    wvs_df = pd.read_csv(wvs_filepath)\n",
    "    _df = pd.read_csv(filepath)\n",
    "    \n",
    "    demographic_mapping_responses = get_demographic_mapping()\n",
    "    demographic_mapping_wvs = get_demographic_mapping(year)\n",
    "\n",
    "    rename_map = {col: col.split(':')[0].strip() for col in wvs_df.columns if ':' in col}\n",
    "    wvs_df.rename(columns=rename_map, inplace=True)\n",
    "    _df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    wvs_df.rename(columns=demographic_mapping_wvs, inplace=True)\n",
    "    _df.rename(columns=demographic_mapping_responses, inplace=True)\n",
    "\n",
    "    rename_map_v_to_q = {}\n",
    "    for col in wvs_df.columns:\n",
    "        if col.startswith('V'):\n",
    "            rename_map_v_to_q[col] = f\"Q{col[1:]}\"\n",
    "    wvs_df.rename(columns=rename_map_v_to_q, inplace=True)\n",
    "\n",
    "    not_scale_questions = [\"Q42\", \"Q90\", \"Q149\", \"Q150\", \"Q151\"]\n",
    "    demographic_cols = list(demographic_mapping_wvs.values())\n",
    "    selected_questions = [q for q in _df.columns if q.startswith('Q')]\n",
    "\n",
    "    for df in [wvs_df, _df]:\n",
    "        for col in selected_questions:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(\n",
    "                    lambda x: flat_answer_mapping.get(str(x).strip(), x) if isinstance(x, str) else x\n",
    "                )\n",
    "\n",
    "    # Map town size to urban/rural\n",
    "    townsize_map = {\n",
    "        \"Under 5,000\": \"Rural\",\n",
    "        \"5000-20000\": \"Urban\",\n",
    "        \"20000-100000\": \"Urban\"\n",
    "    }\n",
    "    wvs_df['urban_rural'] = wvs_df['urban_rural'].map(townsize_map)\n",
    "    age_bins = [0, 15, 24, 34, 44, 54, 64, 100]\n",
    "    age_labels = ['0-15', '16-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "    wvs_df['age'] = pd.cut(wvs_df['age'], bins=age_bins, labels=age_labels, right=True)\n",
    "    \n",
    "    print(wvs_df.columns)\n",
    "    print(_df.columns)\n",
    "\n",
    "    merge_cols = ['region', 'urban_rural', 'age', 'gender', 'marital_status', 'education_level', 'social_class']\n",
    "\n",
    "    print(\"Merge columns:\", merge_cols)\n",
    "    for col in merge_cols:\n",
    "        wvs_df[col] = wvs_df[col].astype(str)\n",
    "        _df[col] = _df[col].astype(str)\n",
    "    merged_df = pd.merge(wvs_df, _df, on=merge_cols, how='inner')\n",
    "\n",
    "    if len(merged_df) == 0:\n",
    "        print(\"triggered\")\n",
    "        return {}\n",
    "\n",
    "    if region_wise:\n",
    "        results_by_region = {}\n",
    "        if 'region' not in merged_df.columns:\n",
    "            raise ValueError(\"Region column not found in merged data. Cannot perform region-wise analysis.\")\n",
    "            \n",
    "        unique_regions = merged_df['region'].unique()\n",
    "        \n",
    "        for region in unique_regions:\n",
    "            region_df = merged_df[merged_df['region'] == region]\n",
    "            \n",
    "            hard_metric_scores, soft_metric_scores = [], []\n",
    "            for q in selected_questions:\n",
    "                survey_col, _col = f\"{q}_x\", f\"{q}_y\"\n",
    "                if survey_col not in region_df.columns or _col not in region_df.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Perform calculation on the region-specific dataframe\n",
    "                survey_answers = pd.to_numeric(region_df[survey_col], errors='coerce')\n",
    "                model_answers = pd.to_numeric(region_df[_col], errors='coerce')\n",
    "                valid_indices = (survey_answers.notna()) & (model_answers.notna()) & (survey_answers >= 0)\n",
    "                if not valid_indices.any(): continue\n",
    "                \n",
    "                survey_answers = survey_answers[valid_indices]\n",
    "                model_answers = model_answers[valid_indices]\n",
    "\n",
    "                # Metric Logic (same as before)\n",
    "                if metric_type == 'hard':\n",
    "                    scores = (survey_answers == model_answers).astype(int)\n",
    "                    hard_metric_scores.extend(scores)\n",
    "                else:\n",
    "                    if q in not_scale_questions:\n",
    "                        scores = (survey_answers == model_answers).astype(int)\n",
    "                        soft_metric_scores.extend(scores)\n",
    "                    else:\n",
    "                        num_options = num_options_map.get(q)\n",
    "                        if not num_options or num_options <= 1: continue\n",
    "                        error = np.abs(survey_answers - model_answers)\n",
    "                        normalized_error = error / (num_options - 1)\n",
    "                        scores = 1 - normalized_error\n",
    "                        soft_metric_scores.extend(scores)\n",
    "            \n",
    "            # Store results for the current region\n",
    "            region_results = {}\n",
    "            if hard_metric_scores: region_results['hard_metric'] = np.mean(hard_metric_scores)\n",
    "            if soft_metric_scores: region_results['soft_metric_unified'] = np.mean(soft_metric_scores)\n",
    "            results_by_region[region] = region_results\n",
    "            \n",
    "        return results_by_region\n",
    "\n",
    "    else: \n",
    "        hard_metric_scores, soft_metric_scores = [], []\n",
    "        for q in selected_questions:\n",
    "            survey_col, _col = f\"{q}_x\", f\"{q}_y\"\n",
    "            if survey_col not in merged_df.columns or _col not in merged_df.columns: continue\n",
    "            survey_answers = pd.to_numeric(merged_df[survey_col], errors='coerce')\n",
    "            model_answers = pd.to_numeric(merged_df[_col], errors='coerce')\n",
    "            valid_indices = (survey_answers.notna()) & (model_answers.notna()) & (survey_answers >= 0)\n",
    "            if not valid_indices.any(): continue\n",
    "            survey_answers = survey_answers[valid_indices]\n",
    "            model_answers = model_answers[valid_indices]\n",
    "\n",
    "            if metric_type == 'hard':\n",
    "                scores = (survey_answers == model_answers).astype(int)\n",
    "                hard_metric_scores.extend(scores)\n",
    "            else:\n",
    "                if q in not_scale_questions:\n",
    "                    scores = (survey_answers == model_answers).astype(int)\n",
    "                    soft_metric_scores.extend(scores)\n",
    "                else:\n",
    "                    num_options = num_options_map.get(q)\n",
    "                    if not num_options or num_options <= 1: continue\n",
    "                    error = np.abs(survey_answers - model_answers)\n",
    "                    normalized_error = error / (num_options - 1)\n",
    "                    scores = 1 - normalized_error\n",
    "                    soft_metric_scores.extend(scores)\n",
    "\n",
    "        results = {}\n",
    "        if hard_metric_scores: results['hard_metric'] = np.mean(hard_metric_scores)\n",
    "        if soft_metric_scores: results['soft_metric_unified'] = np.mean(soft_metric_scores)\n",
    "        if not results: print(\"\\nNo scores were calculated.\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58e129bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         State             Region  soft_metric_unified  hard_metric\n",
      "0       bengal  IN-WB West Bengal             0.543445     0.301829\n",
      "1    telengana    IN-TG Telangana             0.454215     0.324138\n",
      "2       punjab       IN-PB Punjab             0.544655     0.335443\n",
      "3  maharashtra  IN-MH Maharashtra             0.573993     0.294014\n",
      "4      haryana      IN-HR Haryana             0.507389     0.256158\n",
      "5        delhi        IN-DL Delhi             0.434546     0.262431\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "states = ['bengal', 'telengana', 'punjab', 'maharashtra', 'haryana', 'delhi']\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for state in states:\n",
    "    results_soft = analyze_survey_alignment(state=state, region_wise=True)\n",
    "    results_hard = analyze_survey_alignment(state=state, metric_type='hard', region_wise=True)\n",
    "\n",
    "    # Convert dicts to DataFrames\n",
    "    df_soft = pd.DataFrame.from_dict(results_soft, orient='index')\n",
    "    df_soft.index.name = 'Region'\n",
    "    df_soft.reset_index(inplace=True)\n",
    "\n",
    "    df_hard = pd.DataFrame.from_dict(results_hard, orient='index')\n",
    "    df_hard.index.name = 'Region'\n",
    "    df_hard.reset_index(inplace=True)\n",
    "\n",
    "    # Merge soft and hard metrics side by side\n",
    "    df_combined = pd.merge(df_soft, df_hard, on='Region', how='outer', suffixes=('_soft', '_hard'))\n",
    "    df_combined['State'] = state\n",
    "    all_results.append(df_combined)\n",
    "\n",
    "# Concatenate all states\n",
    "final_table = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Reorder columns\n",
    "final_table = final_table[['State', 'Region', 'soft_metric_unified', 'hard_metric']]\n",
    "\n",
    "# Display\n",
    "print(final_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22687b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['year', 'country', 'region', 'urban_rural', 'gender', 'age', 'language',\n",
      "       'marital_status', 'education_level', 'social_class',\n",
      "       ...\n",
      "       'I_DIVORLIB', 'CHOICE', 'I_VOICE1', 'I_VOICE2', 'I_VOI2_00', 'QOICE',\n",
      "       'LNG_NUM', 'FW_START', 'FW_END', 'Qmode'],\n",
      "      dtype='object', length=290)\n",
      "Index(['language', 'marital_status', 'gender', 'urban_rural', 'region', 'age',\n",
      "       'education_level', 'social_class', 'Q106', 'Q107', 'Q108', 'Q110',\n",
      "       'Q150', 'Q151', 'Q158', 'Q159', 'Q160', 'Q161', 'Q162', 'Q176', 'Q177',\n",
      "       'Q178', 'Q179', 'Q180', 'Q181', 'Q184', 'Q185', 'Q187', 'Q188', 'Q189',\n",
      "       'Q190', 'Q191', 'Q192', 'Q194', 'Q195', 'Q224', 'Q225', 'Q227', 'Q228',\n",
      "       'Q229', 'Q230', 'Q231', 'Q232', 'Q233', 'Q234', 'Q235', 'Q236', 'Q238',\n",
      "       'Q239', 'Q241', 'Q242', 'Q244', 'Q247', 'Q248', 'Q33', 'Q34', 'Q35',\n",
      "       'Q37', 'Q38', 'Q39', 'Q40', 'Q41', 'Q42', 'Q43', 'Q44', 'Q45', 'Q64',\n",
      "       'Q66', 'Q67', 'Q68', 'Q72', 'Q73', 'Q74', 'Q75', 'Q77', 'Q78', 'Q79',\n",
      "       'Q80', 'Q81', 'Q82', 'Q83', 'Q84', 'Q85', 'Q86', 'Q87', 'Q88', 'Q89',\n",
      "       'Q90'],\n",
      "      dtype='object')\n",
      "Merge columns: ['region', 'urban_rural', 'age', 'gender', 'marital_status', 'education_level', 'social_class']\n",
      "triggered\n",
      "Index(['year', 'country', 'region', 'urban_rural', 'gender', 'age', 'language',\n",
      "       'marital_status', 'education_level', 'social_class',\n",
      "       ...\n",
      "       'I_DIVORLIB', 'CHOICE', 'I_VOICE1', 'I_VOICE2', 'I_VOI2_00', 'QOICE',\n",
      "       'LNG_NUM', 'FW_START', 'FW_END', 'Qmode'],\n",
      "      dtype='object', length=290)\n",
      "Index(['language', 'marital_status', 'gender', 'urban_rural', 'region', 'age',\n",
      "       'education_level', 'social_class', 'Q106', 'Q107', 'Q108', 'Q110',\n",
      "       'Q150', 'Q151', 'Q158', 'Q159', 'Q160', 'Q161', 'Q162', 'Q176', 'Q177',\n",
      "       'Q178', 'Q179', 'Q180', 'Q181', 'Q184', 'Q185', 'Q187', 'Q188', 'Q189',\n",
      "       'Q190', 'Q191', 'Q192', 'Q194', 'Q195', 'Q224', 'Q225', 'Q227', 'Q228',\n",
      "       'Q229', 'Q230', 'Q231', 'Q232', 'Q233', 'Q234', 'Q235', 'Q236', 'Q238',\n",
      "       'Q239', 'Q241', 'Q242', 'Q244', 'Q247', 'Q248', 'Q33', 'Q34', 'Q35',\n",
      "       'Q37', 'Q38', 'Q39', 'Q40', 'Q41', 'Q42', 'Q43', 'Q44', 'Q45', 'Q64',\n",
      "       'Q66', 'Q67', 'Q68', 'Q72', 'Q73', 'Q74', 'Q75', 'Q77', 'Q78', 'Q79',\n",
      "       'Q80', 'Q81', 'Q82', 'Q83', 'Q84', 'Q85', 'Q86', 'Q87', 'Q88', 'Q89',\n",
      "       'Q90'],\n",
      "      dtype='object')\n",
      "Merge columns: ['region', 'urban_rural', 'age', 'gender', 'marital_status', 'education_level', 'social_class']\n",
      "triggered\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "results_soft = analyze_survey_alignment(state=state, year='2012', region_wise=True)\n",
    "results_hard = analyze_survey_alignment(state=state, year='2012', metric_type='hard', region_wise=True)\n",
    "print(results_soft)\n",
    "print(results_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d1dd981",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['soft_metric_unified', 'hard_metric'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m final_table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(all_results, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Reorder columns\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m final_table \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mState\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRegion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoft_metric_unified\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhard_metric\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Display\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_table)\n",
      "File \u001b[0;32m~/code/wikipedia-bias-2/env/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4112\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4113\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4115\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/code/wikipedia-bias-2/env/lib/python3.10/site-packages/pandas/core/indexes/base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/code/wikipedia-bias-2/env/lib/python3.10/site-packages/pandas/core/indexes/base.py:6264\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6263\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6264\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['soft_metric_unified', 'hard_metric'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "states = ['bengal', 'telengana', 'punjab', 'maharashtra', 'haryana', 'delhi']\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for state in states:\n",
    "    results_soft = analyze_survey_alignment(state=state, year='2012', region_wise=True)\n",
    "    results_hard = analyze_survey_alignment(state=state, year='2012', metric_type='hard', region_wise=True)\n",
    "\n",
    "    # Convert dicts to DataFrames\n",
    "    df_soft = pd.DataFrame.from_dict(results_soft, orient='index')\n",
    "    df_soft.index.name = 'Region'\n",
    "    df_soft.reset_index(inplace=True)\n",
    "\n",
    "    df_hard = pd.DataFrame.from_dict(results_hard, orient='index')\n",
    "    df_hard.index.name = 'Region'\n",
    "    df_hard.reset_index(inplace=True)\n",
    "\n",
    "    # Merge soft and hard metrics side by side\n",
    "    df_combined = pd.merge(df_soft, df_hard, on='Region', how='outer', suffixes=('_soft', '_hard'))\n",
    "    df_combined['State'] = state\n",
    "    all_results.append(df_combined)\n",
    "\n",
    "# Concatenate all states\n",
    "final_table = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Reorder columns\n",
    "final_table = final_table[['State', 'Region', 'soft_metric_unified', 'hard_metric']]\n",
    "\n",
    "# Display\n",
    "print(final_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
