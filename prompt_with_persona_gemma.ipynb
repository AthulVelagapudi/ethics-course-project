{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22423706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a0e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '2022_India_persona_groups_cleaned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c646551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 74, Columns: 11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"data/{filename}.csv\")\n",
    "\n",
    "rows, cols = df.shape\n",
    "print(f\"Rows: {rows}, Columns: {cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af52b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in the questionnaire -  80\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data/questions.json\", \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "with open(\"data/chosen_cols_gemma.json\", \"r\") as f:\n",
    "    chosen_cols = json.load(f)\n",
    "\n",
    "chosen_qsns = {}\n",
    "\n",
    "for qsn in questions:\n",
    "    if chosen_cols['chosen_cols'][qsn] == True and questions[qsn]['description'] not in chosen_cols['persona_cols']:\n",
    "        chosen_qsns[qsn] = questions[qsn]\n",
    "\n",
    "print(\"Number of questions in the questionnaire - \", len(chosen_qsns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450390f",
   "metadata": {},
   "source": [
    "## Chosen Persona Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efea07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'N_REGION_ISO: Region ISO 3166-2'\n",
    "urban_rural = 'H_URBRURAL: Urban-Rural'\n",
    "age = 'X003R: Age recoded (6 intervals)'\n",
    "gender = 'Q260: Sex'\n",
    "language = 'Q272: Language at home'\n",
    "marital_status = 'Q273: Marital status'\n",
    "education_level = 'Q275R: Highest educational level: Respondent (recoded into 3 groups)'\n",
    "social_class = 'Q287: Social class (subjective)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bad1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_prompt_en = '''\n",
    "Imagine you are a {language}-speaking {marital_status} {gender} from {urban_rural} {region}, India. You are in {age} years of age category and have completed {education_level} education level. You consider yourself part of the {social_class}. Answer the following question from this perspective. Others will read what you choose; your goal is to convince them it was chosen from the perspective of the persona described above.\n",
    "\n",
    "Select exactly one option. Answer ONLY with the number corresponding to the question, followed by the number corresponding to the chosen option. Do NOT repeat the question or any other text.\n",
    "'''\n",
    "\n",
    "user_prompt_en = '''\n",
    "Q: {Question}\n",
    "Options: {Options}\n",
    "A:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6905963f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A_YEAR: Year of survey                                                                2023\n",
       "B_COUNTRY: ISO 3166-1 numeric country code                                           India\n",
       "N_REGION_ISO: Region ISO 3166-2                                                IN-BR Bihar\n",
       "H_URBRURAL: Urban-Rural                                                              Rural\n",
       "Q260: Sex                                                                           Female\n",
       "X003R: Age recoded (6 intervals)                                                     16-24\n",
       "Q272: Language at home                                                               Hindi\n",
       "Q273: Marital status                                                                Single\n",
       "Q275R: Highest educational level: Respondent (recoded into 3 groups)                Higher\n",
       "Q287: Social class (subjective)                                         Lower middle class\n",
       "Counts                                                                                   4\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe2a1dd",
   "metadata": {},
   "source": [
    "## Prompting gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e2088e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad6685ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f400af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... This might take a moment.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8402d8d9032042bcb564b928fa6198fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Local Model ---\n",
    "# This section replaces the initial Gemini API setup.\n",
    "# Make sure to update the path to where your model is stored.\n",
    "model_path = \"/assets/models/google-gemma-3-it-27b\" \n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "print(\"Loading model... This might take a moment.\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",  # Automatically uses your GPUs\n",
    "    torch_dtype=torch.bfloat16 # Efficient data type for modern GPUs\n",
    ")\n",
    "print(\"Model loaded successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "371b258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing question batches for respondent 1:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Processing question batches for respondent 1: 100%|██████████| 8/8 [02:33<00:00, 19.24s/it]\n",
      "Processing question batches for respondent 2: 100%|██████████| 8/8 [02:18<00:00, 17.25s/it]\n",
      "Processing question batches for respondent 3: 100%|██████████| 8/8 [02:45<00:00, 20.74s/it]\n",
      "Processing question batches for respondent 4: 100%|██████████| 8/8 [02:31<00:00, 18.99s/it]\n",
      "Processing question batches for respondent 5: 100%|██████████| 8/8 [02:31<00:00, 18.97s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 10\n",
    "results = []\n",
    "raw_results = []\n",
    "respondent_number = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    respondent_number += 1\n",
    "    general_context = {\n",
    "        \"language\": row[language],\n",
    "        \"marital_status\": row[marital_status],\n",
    "        \"gender\": row[gender],\n",
    "        \"urban_rural\": row[urban_rural],\n",
    "        \"region\": row[region],\n",
    "        \"age\": row[age],\n",
    "        \"education_level\": row[education_level],\n",
    "        \"social_class\": row[social_class]\n",
    "    }\n",
    "\n",
    "    questions = []\n",
    "    for qsn_key in chosen_qsns:\n",
    "        qsn_text = chosen_qsns[qsn_key]['questions'][0]\n",
    "        options_list = chosen_qsns[qsn_key]['options']\n",
    "        options_text = \"\".join([f\"{idx+1}. {opt} \" for idx, opt in enumerate(options_list)])\n",
    "        questions.append((qsn_key, qsn_text, options_list, options_text))\n",
    "\n",
    "    respondent_answers = general_context.copy()\n",
    "    debug_output = {\"persona\": general_context, \"questions\": []}\n",
    "\n",
    "    for i in tqdm(range(0, len(questions), batch_size), desc=f\"Processing question batches for respondent {respondent_number}\"):\n",
    "        batch = questions[i:i+batch_size]\n",
    "\n",
    "        user_prompt = \"\"\n",
    "        for idx, (_, q_text, _, opts_text) in enumerate(batch, start=1):\n",
    "            user_prompt += f\"Question {idx}: {q_text}\\nOptions: {opts_text}\\n\"\n",
    "        user_prompt += \"\\nAnswer ONLY with numbers in format: Q1: <option_number>, Q2: <option_number>, ... Do NOT repeat questions.\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": general_prompt_en.format(**general_context)},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        if hasattr(tokenizer, 'apply_chat_template'):\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        else:\n",
    "            system_content = general_prompt_en.format(**general_context)\n",
    "            formatted_prompt = f\"<s>[INST] <<SYS>>\\n{system_content}\\n<</SYS>>\\n\\n{user_prompt} [/INST]\"\n",
    "\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.0,\n",
    "            repetition_penalty=1.2,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            do_sample=False \n",
    "        )\n",
    "\n",
    "        generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        raw_results.append({\n",
    "            \"question_batch\": user_prompt,\n",
    "            \"formatted_prompt\": formatted_prompt,\n",
    "            \"answer_text\": answer_text\n",
    "        })\n",
    "\n",
    "        batch_answers = re.findall(r'Q\\d+:\\s*(\\d+)', answer_text)\n",
    "\n",
    "        for j, (qsn_key, q_text, opts_list, _) in enumerate(batch):\n",
    "            if j < len(batch_answers):\n",
    "                ans_idx = int(batch_answers[j]) - 1\n",
    "                if 0 <= ans_idx < len(opts_list):\n",
    "                    ans_value = opts_list[ans_idx]\n",
    "                else:\n",
    "                    ans_value = \"Invalid answer\"\n",
    "            else:\n",
    "                ans_value = \"No answer\"\n",
    "                \n",
    "            respondent_answers[qsn_key] = ans_value\n",
    "            debug_output[\"questions\"].append({\n",
    "                \"question_key\": qsn_key,\n",
    "                \"question_text\": q_text,\n",
    "                \"options\": opts_list,\n",
    "                \"answer_id\": batch_answers[j] if j < len(batch_answers) else None,\n",
    "                \"answer_value\": ans_value\n",
    "            })\n",
    "\n",
    "    results.append(respondent_answers)\n",
    "    if respondent_number % 10 == 0:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(\"survey_answers_wide.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86fbbf0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'temp/survey_gemma_answers_debug.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1489571/2364602104.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"survey_gemma_answers_wide.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temp/survey_gemma_answers_debug.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'temp/survey_gemma_answers_debug.json'"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"survey_gemma_answers_wide.csv\", index=False)\n",
    "\n",
    "with open(\"temp/survey_gemma_answers_debug.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(raw_results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13475ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
