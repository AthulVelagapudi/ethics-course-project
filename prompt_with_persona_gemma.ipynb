{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22423706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a0e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/india/2022/2022_india_persona_groups_cleaned_te'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c646551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 8, Columns: 11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"{filename}.csv\")\n",
    "\n",
    "rows, cols = df.shape\n",
    "print(f\"Rows: {rows}, Columns: {cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af52b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in the questionnaire -  63\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data/translated_questions/questions_te.json\", \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "with open(\"data/chosen_cols_updated.json\", \"r\") as f:\n",
    "    chosen_cols = json.load(f)\n",
    "\n",
    "chosen_qsns = {}\n",
    "\n",
    "for qsn in questions:\n",
    "    if chosen_cols['chosen_cols'][qsn] == True and questions[qsn]['description'] not in chosen_cols['persona_cols']:\n",
    "        chosen_qsns[qsn] = questions[qsn]\n",
    "\n",
    "print(\"Number of questions in the questionnaire - \", len(chosen_qsns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450390f",
   "metadata": {},
   "source": [
    "## Chosen Persona Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efea07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'N_REGION_ISO: Region ISO 3166-2'\n",
    "urban_rural = 'H_URBRURAL: Urban-Rural'\n",
    "age = 'X003R: Age recoded (6 intervals)'\n",
    "gender = 'Q260: Sex'\n",
    "language = 'Q272: Language at home'\n",
    "marital_status = 'Q273: Marital status'\n",
    "education_level = 'Q275R: Highest educational level: Respondent (recoded into 3 groups)'\n",
    "social_class = 'Q287: Social class (subjective)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_prompt_en = '''\n",
    "Imagine you are a {language}-speaking {marital_status} {gender} from {urban_rural} {region}, India. You are in {age} years of age category and have completed {education_level} education level. You consider yourself part of the {social_class}. Answer the following question from this perspective. Others will read what you choose; your goal is to convince them it was chosen from the perspective of the persona described above.\n",
    "\n",
    "Select exactly one option. Answer ONLY with the number corresponding to the question, followed by the number corresponding to the chosen option. Do NOT repeat the question or any other text.\n",
    "'''\n",
    "\n",
    "user_prompt_en = '''\n",
    "Q: {Question}\n",
    "Options: {Options}\n",
    "A:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06c268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fe2a1dd",
   "metadata": {},
   "source": [
    "## Prompting gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad6685ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f400af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... This might take a moment.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d77120fe07a4b108eaa503f2fc3cf99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Local Model ---\n",
    "# This section replaces the initial Gemini API setup.\n",
    "# Make sure to update the path to where your model is stored.\n",
    "# model_path = \"/assets/models/google-gemma-3-it-27b\" \n",
    "model_path = \"/assets/models/google-gemma-3-it-12b\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "print(\"Loading model... This might take a moment.\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16 \n",
    ")\n",
    "print(\"Model loaded successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing question batches for respondent 1:   0%|          | 0/51 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Processing question batches for respondent 1: 100%|██████████| 51/51 [09:57<00:00, 11.72s/it]\n",
      "Processing question batches for respondent 2: 100%|██████████| 51/51 [09:57<00:00, 11.72s/it]\n",
      "Processing question batches for respondent 3: 100%|██████████| 51/51 [09:58<00:00, 11.73s/it]\n",
      "Processing question batches for respondent 4: 100%|██████████| 51/51 [09:58<00:00, 11.73s/it]\n",
      "Processing question batches for respondent 5: 100%|██████████| 51/51 [09:58<00:00, 11.73s/it]\n",
      "Processing question batches for respondent 6: 100%|██████████| 51/51 [09:58<00:00, 11.73s/it]\n",
      "Processing question batches for respondent 7: 100%|██████████| 51/51 [09:58<00:00, 11.73s/it]\n",
      "Processing question batches for respondent 8: 100%|██████████| 51/51 [09:58<00:00, 11.73s/it]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "batch_size = 5\n",
    "results = []\n",
    "raw_results = []\n",
    "respondent_number = 0\n",
    "\n",
    "# regex to extract pairs like \"Q1: 2\" (case-insensitive)\n",
    "answer_pattern = re.compile(r'Q\\s*(\\d+)\\s*[:\\-]\\s*([0-9]+)', re.IGNORECASE)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    respondent_number += 1\n",
    "    general_context = {\n",
    "        \"language\": row[language],\n",
    "        \"marital_status\": row[marital_status],\n",
    "        \"gender\": row[gender],\n",
    "        \"urban_rural\": row[urban_rural],\n",
    "        \"region\": row[region],\n",
    "        \"age\": row[age],\n",
    "        \"education_level\": row[education_level],\n",
    "        \"social_class\": row[social_class]\n",
    "    }\n",
    "\n",
    "    # prepare questions list - include all 4 question variants\n",
    "    questions = []\n",
    "    for qsn_key in chosen_qsns:\n",
    "        options_list = chosen_qsns[qsn_key]['options']\n",
    "        options_text = \"\".join([f\"{idx+1}. {opt} \" for idx, opt in enumerate(options_list)])\n",
    "        \n",
    "        # Add all 4 question variants (0, 1, 2, 3)\n",
    "        for qsn_variant in range(4):\n",
    "            if qsn_variant < len(chosen_qsns[qsn_key]['questions']):\n",
    "                qsn_text = chosen_qsns[qsn_key]['questions'][qsn_variant]\n",
    "                questions.append((qsn_key, qsn_text, options_list, options_text, qsn_variant))\n",
    "            else:\n",
    "                # If there are fewer than 4 variants, break\n",
    "                break\n",
    "\n",
    "    respondent_answers = general_context.copy()\n",
    "    debug_output = {\"persona\": general_context, \"questions\": []}\n",
    "\n",
    "    for i in tqdm(range(0, len(questions), batch_size), desc=f\"Processing question batches for respondent {respondent_number}\"):\n",
    "        batch = questions[i:i+batch_size]\n",
    "\n",
    "    \n",
    "        user_prompt = \"\"\n",
    "        for idx, (_, q_text, _, opts_text, qsn_variant) in enumerate(batch, start=1):\n",
    "            user_prompt += f\"Question {idx}: {q_text}\\nOptions: {opts_text}\\n\"\n",
    "        user_prompt += \"\\nAnswer ONLY with numbers in format: Q1: <option_number>, Q2: <option_number>, ... Do NOT repeat questions.\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": general_prompt_en.format(**general_context)},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        if hasattr(tokenizer, 'apply_chat_template'):\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        else:\n",
    "            system_content = general_prompt_en.format(**general_context)\n",
    "            formatted_prompt = f\"<s>[INST] <<SYS>>\\n{system_content}\\n<</SYS>>\\n\\n{user_prompt} [/INST]\"\n",
    "\n",
    "\n",
    "        tokenizer_max_len = getattr(tokenizer, \"model_max_length\", None)\n",
    "        if tokenizer_max_len is None or tokenizer_max_len > 100000:\n",
    "            max_input_len = 2048  # Safe default\n",
    "        else:\n",
    "            max_input_len = min(tokenizer_max_len, 2048)  # Cap at 2048 for safety\n",
    "            \n",
    "        # print(\"INPUT PROMPT:\\n\", formatted_prompt)\n",
    "            \n",
    "        inputs = tokenizer(\n",
    "            formatted_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_input_len,\n",
    "            padding=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        # ===== FIX: remove temperature if not sampling (do_sample=False) =====\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            repetition_penalty=1.2,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,           \n",
    "        )\n",
    "\n",
    "        prompt_len = inputs['input_ids'].shape[1]\n",
    "        # outputs is tensor (batch, seq_len)\n",
    "        generated_tokens = outputs[0, prompt_len:]\n",
    "        answer_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "        # print(\"Model Answer:\\n\")\n",
    "        # print(answer_text)\n",
    "        # break\n",
    "        raw_results.append({\n",
    "            \"question_batch\": user_prompt,\n",
    "            \"formatted_prompt\": formatted_prompt,\n",
    "            \"answer_text\": answer_text\n",
    "        })\n",
    "\n",
    "        # ===== more robust parsing =====\n",
    "        matches = answer_pattern.findall(answer_text)\n",
    "        # matches -> list of (qnum_str, ansnum_str)\n",
    "        answers_by_qnum = {int(q): int(a) for q, a in matches}\n",
    "\n",
    "        # now map answers to the batch questions\n",
    "        for j, (qsn_key, q_text, opts_list, _, qsn_variant) in enumerate(batch):\n",
    "            q_num_in_batch = j + 1 \n",
    "            if q_num_in_batch in answers_by_qnum:\n",
    "                ans_idx = answers_by_qnum[q_num_in_batch] - 1\n",
    "                if 0 <= ans_idx < len(opts_list):\n",
    "                    ans_value = opts_list[ans_idx]\n",
    "                    ans_id = answers_by_qnum[q_num_in_batch]\n",
    "                else:\n",
    "                    ans_value = \"Invalid answer\"\n",
    "                    ans_id = answers_by_qnum[q_num_in_batch]\n",
    "            else:\n",
    "                ans_value = \"No answer\"\n",
    "                ans_id = None\n",
    "\n",
    "            variant_key = f\"{qsn_key}_variant_{qsn_variant}\"\n",
    "            respondent_answers[variant_key] = ans_value\n",
    "            debug_output[\"questions\"].append({\n",
    "                \"question_key\": qsn_key,\n",
    "                \"question_variant\": qsn_variant,\n",
    "                \"question_text\": q_text,\n",
    "                \"options\": opts_list,\n",
    "                \"answer_id\": ans_id,\n",
    "                \"answer_value\": ans_value\n",
    "            })\n",
    "\n",
    "    results.append(respondent_answers)\n",
    "    if respondent_number % 10 == 0:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(\"survey_answers_wide_gemma_telugu.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbbf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"gemma_survey_answers_wide_telugu1.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
