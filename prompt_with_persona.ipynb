{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22423706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a0e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '2022_India_persona_groups_cleaned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c646551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 74, Columns: 11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"data/{filename}.csv\")\n",
    "\n",
    "rows, cols = df.shape\n",
    "print(f\"Rows: {rows}, Columns: {cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af52b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in the questionnaire -  122\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data/questions.json\", \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "with open(\"data/chosen_cols.json\", \"r\") as f:\n",
    "    chosen_cols = json.load(f)\n",
    "\n",
    "chosen_qsns = {}\n",
    "\n",
    "for qsn in questions:\n",
    "    if chosen_cols['chosen_cols'][qsn] == True and questions[qsn]['description'] not in chosen_cols['persona_cols']:\n",
    "        chosen_qsns[qsn] = questions[qsn]\n",
    "\n",
    "print(\"Number of questions in the questionnaire - \", len(chosen_qsns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450390f",
   "metadata": {},
   "source": [
    "## Chosen Persona Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efea07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'N_REGION_ISO: Region ISO 3166-2'\n",
    "urban_rural = 'H_URBRURAL: Urban-Rural'\n",
    "age = 'X003R: Age recoded (6 intervals)'\n",
    "gender = 'Q260: Sex'\n",
    "language = 'Q272: Language at home'\n",
    "marital_status = 'Q273: Marital status'\n",
    "education_level = 'Q275R: Highest educational level: Respondent (recoded into 3 groups)'\n",
    "social_class = 'Q287: Social class (subjective)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bad1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_prompt_en = '''\n",
    "Imagine you are a {language}-speaking {marital_status} {gender} from {urban_rural} {region}, India. You are in {age} years of age category and have completed {education_level} education level. You consider yourself part of the {social_class}. Answer the following question from this perspective. Others will read what you choose; your goal is to convince them it was chosen from the perspective of the persona described above.\n",
    "\n",
    "Select exactly one option. Answer ONLY with the number corresponding to the question, followed by the number corresponding to the chosen option. Do NOT repeat the question or any other text.\n",
    "'''\n",
    "\n",
    "user_prompt_en = '''\n",
    "Q: {Question}\n",
    "Options: {Options}\n",
    "A:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe2a1dd",
   "metadata": {},
   "source": [
    "## Prompting LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651f6580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0eb6153fac24a488f5efff4eb72c250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"/assets/models/meta-llama-2-chat-13b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c958f",
   "metadata": {},
   "source": [
    "## 1. LLama2 Chat -> English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caddb863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing question batches for respondent 1:   0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing question batches for respondent 1: 100%|██████████| 13/13 [01:07<00:00,  5.18s/it]\n",
      "Processing question batches for respondent 2: 100%|██████████| 13/13 [01:07<00:00,  5.18s/it]\n",
      "Processing question batches for respondent 3: 100%|██████████| 13/13 [01:10<00:00,  5.45s/it]\n",
      "Processing question batches for respondent 4: 100%|██████████| 13/13 [01:05<00:00,  5.02s/it]\n",
      "Processing question batches for respondent 5: 100%|██████████| 13/13 [01:07<00:00,  5.19s/it]\n",
      "Processing question batches for respondent 6: 100%|██████████| 13/13 [01:07<00:00,  5.20s/it]\n",
      "Processing question batches for respondent 7: 100%|██████████| 13/13 [01:10<00:00,  5.40s/it]\n",
      "Processing question batches for respondent 8: 100%|██████████| 13/13 [01:10<00:00,  5.40s/it]\n",
      "Processing question batches for respondent 9: 100%|██████████| 13/13 [01:10<00:00,  5.42s/it]\n",
      "Processing question batches for respondent 10: 100%|██████████| 13/13 [01:07<00:00,  5.20s/it]\n",
      "Processing question batches for respondent 11: 100%|██████████| 13/13 [01:05<00:00,  5.05s/it]\n",
      "Processing question batches for respondent 12: 100%|██████████| 13/13 [01:05<00:00,  5.05s/it]\n",
      "Processing question batches for respondent 13: 100%|██████████| 13/13 [01:08<00:00,  5.24s/it]\n",
      "Processing question batches for respondent 14: 100%|██████████| 13/13 [01:07<00:00,  5.21s/it]\n",
      "Processing question batches for respondent 15: 100%|██████████| 13/13 [01:10<00:00,  5.45s/it]\n",
      "Processing question batches for respondent 16: 100%|██████████| 13/13 [01:08<00:00,  5.25s/it]\n",
      "Processing question batches for respondent 17: 100%|██████████| 13/13 [01:15<00:00,  5.78s/it]\n",
      "Processing question batches for respondent 18: 100%|██████████| 13/13 [01:11<00:00,  5.46s/it]\n",
      "Processing question batches for respondent 19: 100%|██████████| 13/13 [01:08<00:00,  5.24s/it]\n",
      "Processing question batches for respondent 20: 100%|██████████| 13/13 [01:07<00:00,  5.23s/it]\n",
      "Processing question batches for respondent 21: 100%|██████████| 13/13 [01:07<00:00,  5.22s/it]\n",
      "Processing question batches for respondent 22: 100%|██████████| 13/13 [01:05<00:00,  5.00s/it]\n",
      "Processing question batches for respondent 23: 100%|██████████| 13/13 [01:04<00:00,  4.96s/it]\n",
      "Processing question batches for respondent 24: 100%|██████████| 13/13 [01:07<00:00,  5.17s/it]\n",
      "Processing question batches for respondent 25: 100%|██████████| 13/13 [01:06<00:00,  5.14s/it]\n",
      "Processing question batches for respondent 26: 100%|██████████| 13/13 [01:06<00:00,  5.13s/it]\n",
      "Processing question batches for respondent 27: 100%|██████████| 13/13 [01:03<00:00,  4.91s/it]\n",
      "Processing question batches for respondent 28: 100%|██████████| 13/13 [01:04<00:00,  4.96s/it]\n",
      "Processing question batches for respondent 29: 100%|██████████| 13/13 [01:09<00:00,  5.36s/it]\n",
      "Processing question batches for respondent 30: 100%|██████████| 13/13 [01:09<00:00,  5.37s/it]\n",
      "Processing question batches for respondent 31: 100%|██████████| 13/13 [01:11<00:00,  5.50s/it]\n",
      "Processing question batches for respondent 32: 100%|██████████| 13/13 [01:06<00:00,  5.15s/it]\n",
      "Processing question batches for respondent 33: 100%|██████████| 13/13 [01:06<00:00,  5.13s/it]\n",
      "Processing question batches for respondent 34: 100%|██████████| 13/13 [01:06<00:00,  5.08s/it]\n",
      "Processing question batches for respondent 35: 100%|██████████| 13/13 [01:06<00:00,  5.10s/it]\n",
      "Processing question batches for respondent 36: 100%|██████████| 13/13 [01:04<00:00,  5.00s/it]\n",
      "Processing question batches for respondent 37: 100%|██████████| 13/13 [01:02<00:00,  4.79s/it]\n",
      "Processing question batches for respondent 38: 100%|██████████| 13/13 [01:08<00:00,  5.24s/it]\n",
      "Processing question batches for respondent 39: 100%|██████████| 13/13 [01:07<00:00,  5.19s/it]\n",
      "Processing question batches for respondent 40: 100%|██████████| 13/13 [01:07<00:00,  5.20s/it]\n",
      "Processing question batches for respondent 41: 100%|██████████| 13/13 [01:04<00:00,  4.98s/it]\n",
      "Processing question batches for respondent 42: 100%|██████████| 13/13 [01:04<00:00,  4.98s/it]\n",
      "Processing question batches for respondent 43: 100%|██████████| 13/13 [01:06<00:00,  5.08s/it]\n",
      "Processing question batches for respondent 44: 100%|██████████| 13/13 [01:06<00:00,  5.14s/it]\n",
      "Processing question batches for respondent 45: 100%|██████████| 13/13 [01:03<00:00,  4.88s/it]\n",
      "Processing question batches for respondent 46: 100%|██████████| 13/13 [01:04<00:00,  4.96s/it]\n",
      "Processing question batches for respondent 47: 100%|██████████| 13/13 [01:01<00:00,  4.72s/it]\n",
      "Processing question batches for respondent 48: 100%|██████████| 13/13 [01:03<00:00,  4.91s/it]\n",
      "Processing question batches for respondent 49: 100%|██████████| 13/13 [01:05<00:00,  5.02s/it]\n",
      "Processing question batches for respondent 50: 100%|██████████| 13/13 [01:05<00:00,  5.05s/it]\n",
      "Processing question batches for respondent 51: 100%|██████████| 13/13 [01:02<00:00,  4.80s/it]\n",
      "Processing question batches for respondent 52: 100%|██████████| 13/13 [01:04<00:00,  4.93s/it]\n",
      "Processing question batches for respondent 53: 100%|██████████| 13/13 [01:05<00:00,  5.05s/it]\n",
      "Processing question batches for respondent 54: 100%|██████████| 13/13 [01:05<00:00,  5.05s/it]\n",
      "Processing question batches for respondent 55: 100%|██████████| 13/13 [01:02<00:00,  4.82s/it]\n",
      "Processing question batches for respondent 56: 100%|██████████| 13/13 [01:04<00:00,  4.98s/it]\n",
      "Processing question batches for respondent 57: 100%|██████████| 13/13 [01:05<00:00,  5.06s/it]\n",
      "Processing question batches for respondent 58: 100%|██████████| 13/13 [01:06<00:00,  5.12s/it]\n",
      "Processing question batches for respondent 59: 100%|██████████| 13/13 [01:05<00:00,  5.05s/it]\n",
      "Processing question batches for respondent 60: 100%|██████████| 13/13 [01:02<00:00,  4.84s/it]\n",
      "Processing question batches for respondent 61: 100%|██████████| 13/13 [01:04<00:00,  5.00s/it]\n",
      "Processing question batches for respondent 62: 100%|██████████| 13/13 [01:10<00:00,  5.40s/it]\n",
      "Processing question batches for respondent 63: 100%|██████████| 13/13 [01:06<00:00,  5.13s/it]\n",
      "Processing question batches for respondent 64: 100%|██████████| 13/13 [01:02<00:00,  4.79s/it]\n",
      "Processing question batches for respondent 65: 100%|██████████| 13/13 [01:06<00:00,  5.14s/it]\n",
      "Processing question batches for respondent 66: 100%|██████████| 13/13 [01:06<00:00,  5.14s/it]\n",
      "Processing question batches for respondent 67: 100%|██████████| 13/13 [01:05<00:00,  5.06s/it]\n",
      "Processing question batches for respondent 68: 100%|██████████| 13/13 [01:06<00:00,  5.13s/it]\n",
      "Processing question batches for respondent 69: 100%|██████████| 13/13 [01:01<00:00,  4.76s/it]\n",
      "Processing question batches for respondent 70: 100%|██████████| 13/13 [01:05<00:00,  5.05s/it]\n",
      "Processing question batches for respondent 71: 100%|██████████| 13/13 [01:09<00:00,  5.36s/it]\n",
      "Processing question batches for respondent 72: 100%|██████████| 13/13 [01:06<00:00,  5.13s/it]\n",
      "Processing question batches for respondent 73: 100%|██████████| 13/13 [01:05<00:00,  5.01s/it]\n",
      "Processing question batches for respondent 74: 100%|██████████| 13/13 [01:07<00:00,  5.19s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "batch_size = 10\n",
    "results = []\n",
    "raw_results = []\n",
    "respondent_number = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    respondent_number += 1\n",
    "    general_context = {\n",
    "        \"language\": row[language],\n",
    "        \"marital_status\": row[marital_status],\n",
    "        \"gender\": row[gender],\n",
    "        \"urban_rural\": row[urban_rural],\n",
    "        \"region\": row[region],\n",
    "        \"age\": row[age],\n",
    "        \"education_level\": row[education_level],\n",
    "        \"social_class\": row[social_class]\n",
    "    }\n",
    "\n",
    "    questions = []\n",
    "    for qsn_key in chosen_qsns:\n",
    "        qsn_text = chosen_qsns[qsn_key]['questions'][0]\n",
    "        options_list = chosen_qsns[qsn_key]['options']\n",
    "        options_text = \"\".join([f\"{idx+1}. {opt} \" for idx, opt in enumerate(options_list)])\n",
    "        questions.append((qsn_key, qsn_text, options_list, options_text))\n",
    "\n",
    "    respondent_answers = general_context.copy()\n",
    "    debug_output = {\"persona\": general_context, \"questions\": []}\n",
    "\n",
    "    # === Process in batches ===\n",
    "    for i in tqdm(range(0, len(questions), batch_size), desc=f\"Processing question batches for respondent {respondent_number}\"):\n",
    "        batch = questions[i:i+batch_size]\n",
    "\n",
    "        user_prompt = \"\"\n",
    "        for idx, (_, q_text, _, opts_text) in enumerate(batch, start=1):\n",
    "            user_prompt += f\"Question {idx}: {q_text}\\nOptions: {opts_text}\\n\"\n",
    "        user_prompt += \"\\nAnswer ONLY with numbers in format: Q1: <option_number>, Q2: <option_number>, ... Do NOT repeat questions.\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": general_prompt_en.format(**general_context)},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        if hasattr(tokenizer, 'apply_chat_template'):\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        else:\n",
    "            system_content = general_prompt_en.format(**general_context)\n",
    "            formatted_prompt = f\"<s>[INST] <<SYS>>\\n{system_content}\\n<</SYS>>\\n\\n{user_prompt} [/INST]\"\n",
    "\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.0,\n",
    "            repetition_penalty=1.2,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            do_sample=False \n",
    "        )\n",
    "\n",
    "        generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        raw_results.append({\n",
    "            \"question_batch\": user_prompt,\n",
    "            \"formatted_prompt\": formatted_prompt,\n",
    "            \"answer_text\": answer_text\n",
    "        })\n",
    "\n",
    "        batch_answers = re.findall(r'Q\\d+:\\s*(\\d+)', answer_text)\n",
    "\n",
    "        for j, (qsn_key, q_text, opts_list, _) in enumerate(batch):\n",
    "            if j < len(batch_answers):\n",
    "                ans_idx = int(batch_answers[j]) - 1\n",
    "                if 0 <= ans_idx < len(opts_list):\n",
    "                    ans_value = opts_list[ans_idx]\n",
    "                else:\n",
    "                    ans_value = \"Invalid answer\"\n",
    "            else:\n",
    "                ans_value = \"No answer\"\n",
    "                \n",
    "            respondent_answers[qsn_key] = ans_value\n",
    "            debug_output[\"questions\"].append({\n",
    "                \"question_key\": qsn_key,\n",
    "                \"question_text\": q_text,\n",
    "                \"options\": opts_list,\n",
    "                \"answer_id\": batch_answers[j] if j < len(batch_answers) else None,\n",
    "                \"answer_value\": ans_value\n",
    "            })\n",
    "\n",
    "    results.append(respondent_answers)\n",
    "    if respondent_number % 10 == 0:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(\"survey_answers_wide.csv\", index=False)\n",
    "\n",
    "# Wide-format CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"survey_answers_wide.csv\", index=False)\n",
    "\n",
    "# Debug JSON\n",
    "with open(\"temp/survey_answers_debug.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(raw_results, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
