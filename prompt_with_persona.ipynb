{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22423706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a0e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '2022_India_persona_groups_cleaned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c646551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 74, Columns: 11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"data/{filename}.csv\")\n",
    "\n",
    "rows, cols = df.shape\n",
    "print(f\"Rows: {rows}, Columns: {cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af52b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in the questionnaire -  99\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data/questions.json\", \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "with open(\"data/chosen_cols.json\", \"r\") as f:\n",
    "    chosen_cols = json.load(f)\n",
    "\n",
    "chosen_qsns = {}\n",
    "\n",
    "for qsn in questions:\n",
    "    if chosen_cols['chosen_cols'][qsn] == True and questions[qsn]['description'] not in chosen_cols['persona_cols']:\n",
    "        chosen_qsns[qsn] = questions[qsn]\n",
    "\n",
    "print(\"Number of questions in the questionnaire - \", len(chosen_qsns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450390f",
   "metadata": {},
   "source": [
    "## Chosen Persona Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efea07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'N_REGION_ISO: Region ISO 3166-2'\n",
    "urban_rural = 'H_URBRURAL: Urban-Rural'\n",
    "age = 'X003R: Age recoded (6 intervals)'\n",
    "gender = 'Q260: Sex'\n",
    "language = 'Q272: Language at home'\n",
    "marital_status = 'Q273: Marital status'\n",
    "education_level = 'Q275R: Highest educational level: Respondent (recoded into 3 groups)'\n",
    "social_class = 'Q287: Social class (subjective)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38e65582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punjab: 4\n",
      "Telangana: 8\n",
      "Maharashtra: 6\n",
      "Bihar: 14\n",
      "West Bengal: 4\n"
     ]
    }
   ],
   "source": [
    "df_punjab = df[df[region].str.contains(\"Punjab\", case=False, na=False)].copy()\n",
    "df_telangana = df[df[region].str.contains(\"Telangana\", case=False, na=False)].copy()\n",
    "df_maharashtra = df[df[region].str.contains(\"Maharashtra\", case=False, na=False)].copy()\n",
    "df_bihar = df[df[region].str.contains(\"Bihar\", case=False, na=False)].copy()\n",
    "df_bengal = df[df[region].str.contains(\"West Bengal\", case=False, na=False)].copy()\n",
    "\n",
    "print(\"Punjab:\", len(df_punjab))\n",
    "print(\"Telangana:\", len(df_telangana))\n",
    "print(\"Maharashtra:\", len(df_maharashtra))\n",
    "print(\"Bihar:\", len(df_bihar))\n",
    "print(\"West Bengal:\", len(df_bengal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea3b384e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  N_REGION_ISO: Region ISO 3166-2  count\n",
      "0                     IN-BR Bihar     14\n",
      "1                     IN-DL Delhi     11\n",
      "2                   IN-HR Haryana      3\n",
      "3               IN-MH Maharashtra      6\n",
      "4                    IN-PB Punjab      4\n",
      "5                 IN-TG Telangana      8\n",
      "6             IN-UP Uttar Pradesh     24\n",
      "7               IN-WB West Bengal      4\n"
     ]
    }
   ],
   "source": [
    "region_counts = df.groupby(region).size().reset_index(name=\"count\")\n",
    "print(region_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bad1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_prompt_en = '''\n",
    "Imagine you are a {language}-speaking {marital_status} {gender} from {urban_rural} {region}, India. You are in {age} years of age category and have completed {education_level} education level. You consider yourself part of the {social_class}. Answer the following question from this perspective. Others will read what you choose; your goal is to convince them it was chosen from the perspective of the persona described above.\n",
    "\n",
    "Select exactly one option. Answer ONLY with the number corresponding to the question, followed by the number corresponding to the chosen option. Do NOT repeat the question or any other text.\n",
    "'''\n",
    "\n",
    "user_prompt_en = '''\n",
    "Q: {Question}\n",
    "Options: {Options}\n",
    "A:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe2a1dd",
   "metadata": {},
   "source": [
    "## Prompting LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f6580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be040034dcac4c5c8aaed71cca7b7180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"/assets/models/meta-llama-2-chat-13b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acdf9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c958f",
   "metadata": {},
   "source": [
    "## 1. LLama2 Chat -> English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "caddb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "def find_responses(df, state):\n",
    "    batch_size = 5\n",
    "    results = []\n",
    "    raw_results = []\n",
    "    respondent_number = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        respondent_number += 1\n",
    "        general_context = {\n",
    "            \"language\": row[language],\n",
    "            \"marital_status\": row[marital_status],\n",
    "            \"gender\": row[gender],\n",
    "            \"urban_rural\": row[urban_rural],\n",
    "            \"region\": row[region],\n",
    "            \"age\": row[age],\n",
    "            \"education_level\": row[education_level],\n",
    "            \"social_class\": row[social_class]\n",
    "        }\n",
    "\n",
    "        questions = []\n",
    "        for qsn_key in chosen_qsns:\n",
    "            for qsn_instance in range(0,4):\n",
    "                if len(chosen_qsns[qsn_key]['questions']) <= qsn_instance:\n",
    "                    break\n",
    "                qsn_text = chosen_qsns[qsn_key]['questions'][qsn_instance]\n",
    "                options_list = chosen_qsns[qsn_key]['options']\n",
    "                options_text = \"\".join([f\"{idx+1}. {opt} \" for idx, opt in enumerate(options_list)])\n",
    "                questions.append((qsn_key, qsn_text, options_list, options_text, qsn_instance))\n",
    "\n",
    "        respondent_answers = general_context.copy()\n",
    "        \n",
    "        # === Process in batches ===\n",
    "        for i in tqdm(range(0, len(questions), batch_size), desc=f\"Processing question batches for respondent {respondent_number}\"):\n",
    "            batch = questions[i:i+batch_size]\n",
    "\n",
    "            user_prompt = \"\"\n",
    "            for idx, (_, q_text, _, opts_text, qsn_instance) in enumerate(batch, start=1):\n",
    "                user_prompt += f\"Question {idx}: {q_text}\\nOptions: {opts_text}\\n\"\n",
    "            user_prompt += \"\\nAnswer ONLY with numbers in format: Q1: <option_number>, Q2: <option_number>, ... Do NOT repeat questions.\"\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": general_prompt_en.format(**general_context)},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "\n",
    "            if hasattr(tokenizer, 'apply_chat_template'):\n",
    "                formatted_prompt = tokenizer.apply_chat_template(\n",
    "                    messages, \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "            else:\n",
    "                system_content = general_prompt_en.format(**general_context)\n",
    "                formatted_prompt = f\"<s>[INST] <<SYS>>\\n{system_content}\\n<</SYS>>\\n\\n{user_prompt} [/INST]\"\n",
    "\n",
    "            inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.0,\n",
    "                repetition_penalty=1.2,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id, \n",
    "                do_sample=False \n",
    "            )\n",
    "\n",
    "            generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "            answer_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "            raw_results.append({\n",
    "                \"question_batch\": user_prompt,\n",
    "                \"formatted_prompt\": formatted_prompt,\n",
    "                \"answer_text\": answer_text\n",
    "            })\n",
    "\n",
    "            batch_answers = re.findall(r'Q\\d+:\\s*(\\d+)', answer_text)\n",
    "\n",
    "            for j, (qsn_key, q_text, opts_list, _, qsn_instance) in enumerate(batch):\n",
    "                if j < len(batch_answers):\n",
    "                    ans_idx = int(batch_answers[j]) - 1\n",
    "                    if 0 <= ans_idx < len(opts_list):\n",
    "                        ans_value = opts_list[ans_idx]\n",
    "                    else:\n",
    "                        ans_value = \"Invalid answer\"\n",
    "                else:\n",
    "                    ans_value = \"No answer\"\n",
    "                    \n",
    "                respondent_answers[str(qsn_key) + ' - ' + str(qsn_instance)] = ans_value\n",
    "\n",
    "        results.append(respondent_answers)\n",
    "        \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f\"responses/survey_answers_{state}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91e584a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing question batches for respondent 1: 100%|██████████| 80/80 [05:06<00:00,  3.83s/it]\n",
      "Processing question batches for respondent 2: 100%|██████████| 80/80 [04:28<00:00,  3.36s/it]\n",
      "Processing question batches for respondent 3: 100%|██████████| 80/80 [04:33<00:00,  3.42s/it]\n",
      "Processing question batches for respondent 4: 100%|██████████| 80/80 [04:32<00:00,  3.41s/it]\n"
     ]
    }
   ],
   "source": [
    "find_responses(df_bengal, \"bengal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2ba8195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing question batches for respondent 1: 100%|██████████| 80/80 [04:23<00:00,  3.30s/it]\n",
      "Processing question batches for respondent 2: 100%|██████████| 80/80 [04:26<00:00,  3.33s/it]\n",
      "Processing question batches for respondent 3: 100%|██████████| 80/80 [04:26<00:00,  3.33s/it]\n",
      "Processing question batches for respondent 4: 100%|██████████| 80/80 [04:15<00:00,  3.19s/it]\n"
     ]
    }
   ],
   "source": [
    "find_responses(df_punjab, \"punjab\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
